{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import RFECV,SelectKBest,mutual_info_classif,SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV,LeaveOneOut,StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,auc,confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether there is missing value in normal mat: False\n",
      "Whether there is missing value in cancer mat: False\n"
     ]
    }
   ],
   "source": [
    "# Case-based difficuty score\n",
    "case_feat_normal = pd.read_csv('/Users/jessy/Desktop/笔记本/Radiomics Data Analysis/diffScore w_out 10 dep Cn/case_feat_diffScore/Square_feat.csv')\n",
    "case_feat_normal = pd.DataFrame(case_feat_normal).drop(\"Unnamed: 0\",axis=1)\n",
    "print('Whether there is missing value in normal mat: {}'.format(case_feat_normal.isnull().values.any())) \n",
    "case_feat_cancer = pd.read_csv('/Users/jessy/Desktop/笔记本/Radiomics Data Analysis/diffScore w_out 10 dep Cn/case_feat_case_cancer_diffScore/Square_feat.csv')\n",
    "case_feat_cancer = pd.DataFrame(case_feat_cancer).drop(\"Unnamed: 0\",axis=1)\n",
    "print('Whether there is missing value in cancer mat: {}'.format(case_feat_cancer.isnull().values.any())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_Normal = case_feat_normal.loc[case_feat_normal['View']==\"'CC'\",]\n",
    "MLO_Normal = case_feat_normal.loc[case_feat_normal['View']==\"'MLO'\",]\n",
    "left_CC = CC_Normal.loc[CC_Normal[\"Side\"]==\"L\",]\n",
    "right_CC =CC_Normal.loc[CC_Normal[\"Side\"]==\"R\",]\n",
    "left_MLO = MLO_Normal.loc[MLO_Normal[\"Side\"]==\"L\",]\n",
    "right_MLO = MLO_Normal.loc[MLO_Normal[\"Side\"]==\"R\",]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesion_side = pd.read_csv(\"/Users/jessy/Desktop/笔记本/Radiomics Data Analysis/diffScore w_out 10 dep Cn/lesion_side.csv\")\n",
    "case_feat_cancer_ = case_feat_cancer.merge(lesion_side, how='left',on = \"CaseName\").drop(\"Unnamed: 0\",axis=1)\n",
    "CC_Cancer = case_feat_cancer_.loc[case_feat_cancer_[\"Side\"]==case_feat_cancer_[\"LesionSide\"] ,].loc[case_feat_cancer_[\"View\"]==\"'CC'\",]\n",
    "MLO_Cancer = case_feat_cancer_.loc[case_feat_cancer_[\"Side\"]==case_feat_cancer_[\"LesionSide\"] ,].loc[case_feat_cancer_[\"View\"]==\"'MLO'\",]\n",
    "\n",
    "CC_Cancer.drop(17, axis=0, inplace=True)\n",
    "MLO_Cancer.drop(23, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of CC_Normal is (80, 210)\n",
      "The shape of CC_CancerLesion is (20, 212)\n",
      "The shape of MLO_Normal is (80, 210)\n",
      "The shape of MLO_CancerLesion is (20, 212)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of CC_Normal is {}'.format(CC_Normal.shape))\n",
    "print('The shape of CC_CancerLesion is {}'.format(CC_Cancer.shape))\n",
    "print('The shape of MLO_Normal is {}'.format(MLO_Normal.shape))\n",
    "print('The shape of MLO_CancerLesion is {}'.format(MLO_Cancer.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algorithms to use: Logistic regression\n",
    "def self_LR_pipe(X,y,pipe, param):\n",
    "    random.seed(10)\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # inner loop for feature selection and hyperparameter tuning \n",
    "        pipe_lr = pipe\n",
    "        param_grid = param\n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        result = GridSearchCV(pipe_lr, param_grid=param, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out validation set\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "        print('Selected features are: {}'.format(np.where(result.best_estimator_.named_steps['features'].get_support())[0]))\n",
    "    \n",
    "        \n",
    "    # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithms to use: Random Forest\n",
    "def self_rf_pipe(X,y,pipe, param):\n",
    "    features = {}\n",
    "    row = 0\n",
    "    random.seed(24)\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred,Predicted_class  = list(),list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        row+=1\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        ## inner loop for feature selection and hyperparameter tuning \n",
    "\n",
    "        cv_inner = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=24)\n",
    "        result = GridSearchCV(pipe, param_grid=param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out evaluation dataset\n",
    "        \n",
    "        yhat_proba = best_param.predict_proba(X_test)[:,1]# reture the probability of predicting '1'\n",
    "        y_pred.append(yhat_proba[0])\n",
    "        Predicted_class.append(best_param.predict(X_test))\n",
    "        y_true.append(y_test[0])\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "        features[row] = np.where(result.best_estimator_.named_steps['features'].get_support())[0]\n",
    "\n",
    "    \n",
    "    Dict = {}\n",
    "    for key in sorted(features):\n",
    "        for num in features[key]:\n",
    "            if num not in Dict:\n",
    "                Dict[num] = 1\n",
    "            else:\n",
    "                Dict[num]+=1\n",
    "    \n",
    "    KEY = []\n",
    "\n",
    "    for key in sorted(Dict):\n",
    "        print(key,':',Dict[key])\n",
    "        if Dict[key] >= len(X)/2:\n",
    "            KEY.append(key)\n",
    "    print(KEY) \n",
    "\n",
    "    # Calculate roc_auc on the hold out dataset\n",
    "    AUC_score = roc_auc_score(y_true, y_pred)\n",
    "    Accuracy = accuracy_score(y_true,Predicted_class)\n",
    "    print('auc: %.3f' % AUC_score)\n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "  \n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "\n",
    "    \n",
    "    cm1 = confusion_matrix(y_true,Predicted_class)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    return Dict,AUC_score,Accuracy,sensitivity1,specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM: to evaluate the entire pipeline\n",
    "def pipeline_evaluate(X,y,pipe, param):\n",
    "    random.seed(24)\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred, Predicted_class  = list(),list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        ## inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=2)\n",
    "        result = GridSearchCV(pipe, param_grid=param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out evaluation dataset\n",
    "        yhat_proba = best_param.predict_proba(X_test)[:,1]# reture the probability of predicting '1'\n",
    "        y_pred.append(yhat_proba[0])\n",
    "        Predicted_class.append(best_param.predict(X_test))\n",
    "        y_true.append(y_test[0])\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "    return y_pred,Predicted_class, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Side_View = {}\n",
    "Side_View = {'left_CC':left_CC,\n",
    "             'right_CC':right_CC,\n",
    "             'left_MLO':left_MLO,\n",
    "             'right_MLO':right_MLO}\n",
    "rads = ['diffScore.CN','diffScore.AU']\n",
    "#Bins = [[0, 0.60, 0.8, 1],[0, 0.69, 0.81, 1]] # whole dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic Regression 02 (log, pca + mutual_info, LR)\n",
    "rad = rads[0]\n",
    "bins = Bins[0]\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0]) # 1 is difficult and 0 is easy group\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202]\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    # prepare data (log transform)\n",
    "    from scipy.stats import skew\n",
    "    skewness = X.apply(lambda x: skew(x))\n",
    "    skewed_feats = skewness[skewness > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    minimum = X[skewed_feats].min(axis = 1).min()\n",
    "    skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    pca = PCA(n_components = 0.9)\n",
    "    selection = SelectKBest(mutual_info_classif,k=10)\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "    pipe_lr2 = Pipeline([\n",
    "        ('scaler',MinMaxScaler()),\n",
    "        ('features',combined_features),\n",
    "        ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "    param_lr2 = {'features__univ_select__k':[5,10,20,30],\n",
    "                'lr__penalty':['l1', 'l2'],\n",
    "                'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "\n",
    "    print('Below is the results from the LR02:')\n",
    "\n",
    "\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        result = GridSearchCV(pipe_lr2, param_lr2, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out validation set\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "       # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Chinese Rads Logistic regression 1 3 \n",
    "print('This is for Chinese Radiologists: \\n')\n",
    "rad = rads[0]\n",
    "bins = Bins[0]\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202]\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    # prepare data (log transform)\n",
    "    from scipy.stats import skew\n",
    "    skewness = X.apply(lambda x: skew(x))\n",
    "    skewed_feats = skewness[skewness > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    minimum = X[skewed_feats].min(axis = 1).min()\n",
    "    skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    # Logistic Regression 01 (log transform, Scaler, RFECV, LR)\n",
    "    pipe01 = Pipeline([('scaler',MinMaxScaler()),\n",
    "                     ('features',RFECV(estimator = LogisticRegression(solver='liblinear'),cv =3, scoring ='roc_auc')),\n",
    "                     ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "    param01 = {'lr__penalty':['l1','l2'],\n",
    "            'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "\n",
    "    print('*'*30)\n",
    "    print('Below is the results from the LR01:')\n",
    "    self_LR_pipe(X,y,pipe01,param01) \n",
    "    print('*'*30)\n",
    "\n",
    "\n",
    "    # Logistic Regression 03 (log transform, scaler, SelectFromModel, LR)\n",
    "    selector = SelectFromModel(estimator=LogisticRegression(solver = 'liblinear'))\n",
    "    pipe_lr03 = Pipeline([ \n",
    "            ('scaler',MinMaxScaler()),\n",
    "            ('features',selector),\n",
    "            ('lr',LogisticRegression(solver = 'liblinear',random_state=42))]) \n",
    "    param_lr03 = {'features__max_features':[20,30,40],\n",
    "                    'lr__penalty':['l1', 'l2'],\n",
    "                    'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "    print('*'*30)\n",
    "    print('Below is the results from the LR03:') \n",
    "    self_LR_pipe(X,y,pipe=pipe_lr03,param=param_lr03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Chinese Rads Random Forest 01\n",
    "rad = rads[0]\n",
    "bins = Bins[0]\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202].to_numpy()\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    # Random forest 01 (scaler, pca + mututal info classifier, RandomForest)\n",
    "    pca = PCA(n_components = 0.9) \n",
    "    selection = SelectKBest(mutual_info_classif,k=10)\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "    pipe = Pipeline([('scaler',MinMaxScaler()),\n",
    "                    ('features',combined_features),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))])\n",
    "\n",
    "    param = {'features__univ_select__k':[10,20,30,40],\n",
    "             'rf__max_features':['sqrt','log2'],\n",
    "             'rf__n_estimators':[50,100,1000,2000]} \n",
    "    print('*'*50)\n",
    "    print('Below is the results from the RF01:')\n",
    "\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        ## inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        result = GridSearchCV(pipe, param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out evaluation dataset\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])# reture the probability of predicting '1'\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "        # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest 02 (scaler, SelectFromModel, RandomForest)\n",
    "# rad = rads[0]\n",
    "# df_cn = dict()\n",
    "# AUC = dict()\n",
    "# ACU = dict()\n",
    "# SEN = dict()\n",
    "# SPE = dict()\n",
    "# #bins = Bins[0]\n",
    "# for key,value in Side_View.items():\n",
    "#     Normal_sort = value.sort_values(by = rad)\n",
    "#     Normal_sort_copy = Normal_sort.copy()\n",
    "#     Normal_sort_copy['percentile']=pd.qcut(Normal_sort_copy[rad],3, labels=[1,3,0])\n",
    "#     Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "#     print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "#     X = Normal_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "#     y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "#     over = SMOTE(sampling_strategy=\"minority\",random_state=2)\n",
    "#     X,y = over.fit_resample(X,y)\n",
    "  \n",
    "\n",
    "#     selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 5)\n",
    "#     pipe = Pipeline([ #('scaler',MinMaxScaler()),\n",
    "#                      ('features',selector),\n",
    "#                      ('rf',RandomForestClassifier(random_state=42))]) \n",
    "#     param = {'features__max_features':[5,10,20],\n",
    "#             \"rf__max_depth\":[1,5,10],\n",
    "#              \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "#                  'rf__n_estimators':[50,100,1000] } \n",
    "#     print('*'*50)\n",
    "#     Dict,AUC_score,Accuracy,sensitivity1,specificity1 = self_rf_pipe(X,y,pipe,param)  \n",
    "#     print('*'*50)\n",
    "#     df_cn[key]=Dict\n",
    "#     AUC[key]=AUC_score\n",
    "#     ACU[key]=Accuracy\n",
    "#     SEN[key]=sensitivity1\n",
    "#     SPE[key]=specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS left_CC SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.840, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.828, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.804, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.843, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.832, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.906, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.868, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.871, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.921, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.881, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.868, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.827, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.844, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.839, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.858, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.859, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.827, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.823, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.806, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.843, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.828, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.815, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.770, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.775, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.802, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.776, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.863, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.6666666666666666\n",
      "The AUC for this view is : 0.7194444444444443\n",
      "The ACC for this view is : 0.6666666666666666\n",
      "The sen for this view is : 0.6666666666666666\n",
      "The spe for this view is : 0.6666666666666666\n",
      "ThIS IS right_CC SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.839, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.832, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.846, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.861, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.840, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.894, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.842, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.872, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.920, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.844, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.815, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.826, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.826, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.837, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.844, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.843, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.804, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.806, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.799, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.860, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.822, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.815, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.748, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.740, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.718, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.778, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.785, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.5\n",
      "The AUC for this view is : 0.7\n",
      "The ACC for this view is : 0.6666666666666666\n",
      "The sen for this view is : 0.8\n",
      "The spe for this view is : 0.5\n",
      "ThIS IS left_MLO SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.929, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.935, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.936, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.929, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.932, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.951, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.942, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.933, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.953, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.935, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.938, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.924, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.947, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">est=0.935, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.918, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.945, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.937, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.937, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.943, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.945, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.936, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.937, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.928, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.929, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.919, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.924, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.981, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      "Specificity :  0.6666666666666666\n",
      "The AUC for this view is : 0.8388888888888889\n",
      "The ACC for this view is : 0.7407407407407407\n",
      "The sen for this view is : 0.8\n",
      "The spe for this view is : 0.6666666666666666\n",
      "ThIS IS right_MLO SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.796, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.815, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.803, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.791, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.814, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.846, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.792, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.818, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.844, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.815, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.779, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.772, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.808, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.863, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.821, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.878, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.833, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.817, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.815, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.860, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.822, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.826, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.784, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.765, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.743, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.715, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.791, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      "Specificity :  0.5\n",
      "The AUC for this view is : 0.6333333333333333\n",
      "The ACC for this view is : 0.6296296296296297\n",
      "The sen for this view is : 0.7333333333333333\n",
      "The spe for this view is : 0.5\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE EVALUATE FOR CHINESE \n",
    "rad = rads[0]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.qcut(Normal_sort_copy[rad],3, labels=[0,3,1])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==0],Normal_sort_copy[Normal_sort_copy['percentile']==1]],axis = 0)\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 5)\n",
    "    pipe = Pipeline([('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,20],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.append(probability)\n",
    "    PRED.append(pre_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.6296296296296297\n",
      "Sensitivity is : 0.4666666666666667\n",
      "Specificity is : 0.8333333333333334\n",
      "AUC for normal pipeline for Chinese readers is 0.817\n",
      "Confidence interval for the score: [0.628 - 0.966]\n"
     ]
    }
   ],
   "source": [
    "pipeline_prob_cn = np.array(PROB).reshape(4,-1)\n",
    "agg_prob_cn = np.max(pipeline_prob_cn, axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(agg_prob_cn)):\n",
    "    if agg_prob_cn[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, agg_prob_cn)\n",
    "print(\"AUC for normal pipeline for Chinese readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(agg_prob_cn), len(agg_prob_cn))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], agg_prob_cn[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Australian Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic Regression 02 (log, pca + mutual_info, LR) \n",
    "rad = rads[1]\n",
    "bins = Bins[1]\n",
    "print('Below is for Australian Radiologists: \\n')\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202]\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    # prepare data (log transform)\n",
    "    from scipy.stats import skew\n",
    "    skewness = X.apply(lambda x: skew(x))\n",
    "    skewed_feats = skewness[skewness > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    minimum = X[skewed_feats].min(axis = 1).min()\n",
    "    skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "    X = X.to_numpy()\n",
    "    pca = PCA(n_components = 0.9)\n",
    "    selection = SelectKBest(mutual_info_classif,k=10)\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "    pipe_lr2 = Pipeline([\n",
    "        ('scaler',MinMaxScaler()),\n",
    "        ('features',combined_features),\n",
    "        ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "    param_lr2 = {'features__univ_select__k':[5,10,20,30],\n",
    "                'lr__penalty':['l1', 'l2'],\n",
    "                'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "\n",
    "    print('Below is the results from the LR02:')\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        result = GridSearchCV(pipe_lr2, param_lr2, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out validation set\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "       # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))       \n",
    "\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Australian Rads Logistic regression 1 3\n",
    "rad = rads[1]\n",
    "bins = Bins[1]\n",
    "print('Below is for Australian Radiologists: \\n')\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202]\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "    # prepare data (log transform)\n",
    "    from scipy.stats import skew\n",
    "    skewness = X.apply(lambda x: skew(x))\n",
    "    skewed_feats = skewness[skewness > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    minimum = X[skewed_feats].min(axis = 1).min()\n",
    "    skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    # Logistic Regression 01 (log transform, Scaler, RFECV, LR)\n",
    "    pipe01 = Pipeline([('scaler',MinMaxScaler()),\n",
    "                     ('features',RFECV(estimator = LogisticRegression(solver='liblinear'),cv =3, scoring ='roc_auc')),\n",
    "                     ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "    param01 = {'lr__penalty':['l1','l2'],\n",
    "            'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "    print('*'*30)\n",
    "    print('Below is the results from the LR01:')\n",
    "    self_LR_pipe(X,y,pipe01,param01)\n",
    "    print('*'*30)\n",
    "\n",
    "\n",
    "    # Logistic Regression 03 (log transform, scaler, SelectFromModel, LR)\n",
    "    selector = SelectFromModel(estimator=LogisticRegression(solver = 'liblinear'))\n",
    "    pipe_lr03 = Pipeline([ \n",
    "            ('scaler',MinMaxScaler()),\n",
    "        ('features',selector),\n",
    "        ('lr',LogisticRegression(solver = 'liblinear',random_state=42))]) \n",
    "    param_lr03 = {'features__max_features':[20,30,40],\n",
    "                    'lr__penalty':['l1', 'l2'],\n",
    "                    'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "    print('*'*30)\n",
    "    print('Below is the results from the LR03:')               \n",
    "    self_LR_pipe(X,y,pipe=pipe_lr03,param=param_lr03)\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# For Australian Rads Random Forest 01\n",
    "rad = rads[1]\n",
    "bins = Bins[1]\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202].to_numpy()\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "    # Random forest 01 (scaler, pca + mututal info classifier, RandomForest)\n",
    "    pca = PCA(n_components = 0.9) \n",
    "    selection = SelectKBest(mutual_info_classif,k=10)\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "    pipe = Pipeline([('scaler',MinMaxScaler()),\n",
    "                    ('features',combined_features),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))])\n",
    "\n",
    "    param = {'features__univ_select__k':[10,20,30,40],\n",
    "             'rf__max_features':['sqrt','log2'],\n",
    "             'rf__n_estimators':[50,100,1000,2000]} \n",
    "\n",
    "\n",
    "    print('*'*50)\n",
    "    print('Below is the results from the RF01:')\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        ## inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        result = GridSearchCV(pipe, param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out evaluation dataset\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])# reture the probability of predicting '1'\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "        # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest 02 (scaler, SelectFromModel, RandomForest)\n",
    "# rad = rads[1]\n",
    "# df_au = dict()\n",
    "# AUC = dict()\n",
    "# ACU = dict()\n",
    "# SEN = dict()\n",
    "# SPE = dict()\n",
    "# #bins = Bins[0]\n",
    "# for key,value in Side_View.items():\n",
    "#     Normal_sort = value.sort_values(by = rad)\n",
    "#     Normal_sort_copy = Normal_sort.copy()\n",
    "#     Normal_sort_copy['percentile']=pd.qcut(Normal_sort_copy[rad],3, labels=[1,3,0])\n",
    "#     Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "#     print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "#     X = Normal_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "#     y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "#     over = SMOTE(sampling_strategy=\"minority\",random_state=2)\n",
    "#     X,y = over.fit_resample(X,y)\n",
    "\n",
    "\n",
    "#     selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 5)\n",
    "#     pipe = Pipeline([ #('scaler',MinMaxScaler()),\n",
    "#                      ('features',selector),\n",
    "#                      ('rf',RandomForestClassifier(random_state=42))]) \n",
    "#     param = {'features__max_features':[5,10,20],\n",
    "#             \"rf__max_depth\":[1,5,10],\n",
    "#              \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "#                  'rf__n_estimators':[50,100,1000] } \n",
    "#     print('*'*50)\n",
    "#     Dict,AUC_score,Accuracy,sensitivity1,specificity1 = self_rf_pipe(X,y,pipe,param)  \n",
    "#     print('*'*50)\n",
    "#     df_au[key]=Dict\n",
    "#     AUC[key]=AUC_score\n",
    "#     ACU[key]=Accuracy\n",
    "#     SEN[key]=sensitivity1\n",
    "#     SPE[key]=specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS left_CC SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.652, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.647, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.695, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.696, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.673, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.678, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.725, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.766, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.674, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.722, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.780, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.829, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.745, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.721, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.741, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.759, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.764, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.751, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.749, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.720, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.749, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.788, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.734, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.714, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.760, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.722, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.686, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.735, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.645, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.622, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      "Specificity :  0.18181818181818182\n",
      "The AUC for this view is : 0.49043062200956944\n",
      "The ACC for this view is : 0.4666666666666667\n",
      "The sen for this view is : 0.631578947368421\n",
      "The spe for this view is : 0.18181818181818182\n",
      "ThIS IS right_CC SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.482, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.416, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.528, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.412, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.437, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.449, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.438, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.455, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.400, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.413, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.429, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.441, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.418, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.411, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.444, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.448, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.478, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.435, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.446, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.445, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.457, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.458, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.532, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.511, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.508, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.455, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.482, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.496, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.463, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.445, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.09090909090909091\n",
      "The AUC for this view is : 0.2583732057416268\n",
      "The ACC for this view is : 0.4666666666666667\n",
      "The sen for this view is : 0.6842105263157895\n",
      "The spe for this view is : 0.09090909090909091\n",
      "ThIS IS left_MLO SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.486, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.548, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.526, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.505, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.491, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.503, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.566, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">est=0.561, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.598, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.545, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.517, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.554, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.552, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.586, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.454, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.506, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.548, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.496, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.561, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.580, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.550, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.604, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.597, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.517, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.629, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.625, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.626, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.644, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.575, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.507, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      "Specificity :  0.09090909090909091\n",
      "The AUC for this view is : 0.3660287081339713\n",
      "The ACC for this view is : 0.5666666666666667\n",
      "The sen for this view is : 0.8421052631578947\n",
      "The spe for this view is : 0.09090909090909091\n",
      "ThIS IS right_MLO SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.519, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.566, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.503, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.485, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.535, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.529, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.504, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.605, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.508, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.528, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.546, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.457, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.521, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.512, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.520, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.512, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.448, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.539, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.508, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.559, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.495, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.540, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.617, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.496, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.593, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.509, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.468, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.588, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.511, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.483, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.18181818181818182\n",
      "The AUC for this view is : 0.34928229665071775\n",
      "The ACC for this view is : 0.5666666666666667\n",
      "The sen for this view is : 0.7894736842105263\n",
      "The spe for this view is : 0.18181818181818182\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE EVALUATE FOR AUSTRALIAN \n",
    "rad = rads[1]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.qcut(Normal_sort_copy[rad],3, labels=[0,3,1])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==0],Normal_sort_copy[Normal_sort_copy['percentile']==1]],axis = 0)\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 5)\n",
    "    pipe = Pipeline([('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,20],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.4\n",
      "Sensitivity is : 0.42105263157894735\n",
      "Specificity is : 0.36363636363636365\n",
      "AUC for normal pipeline for Australian readers is 0.364\n",
      "Confidence interval for the score: [0.165 - 0.59]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(4,-1)\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Australian readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Based \n",
    "### Cancer Lesion Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer lesion side \n",
    "Side_View = {}\n",
    "Side_View = {'Case_based_CC_CancerLesion':CC_Cancer,\n",
    "             'Case_based_MLO_CancerLesion':MLO_Cancer}\n",
    "rads = ['diffScore.CN','diffScore.AU']\n",
    "#Bins = [[0,0.5,0.78,1],[0,0.56,0.90,1]]\n",
    "# For Case Based Cancer diff \n",
    "# Chinese [0.0.55,0.85,1]\n",
    "# Au [0,0.75,0.8750,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Logistic Regression 02\n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202]\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "        # prepare data (log transform)\n",
    "        from scipy.stats import skew\n",
    "        skewness = X.apply(lambda x: skew(x))\n",
    "        skewed_feats = skewness[skewness > 0.75]\n",
    "        skewed_feats = skewed_feats.index\n",
    "        minimum = X[skewed_feats].min(axis = 1).min()\n",
    "        skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "        X = X.to_numpy()  \n",
    "\n",
    "        pipe_lr2 = Pipeline([\n",
    "            ('scaler',MinMaxScaler()),\n",
    "            ('features',combined_features),\n",
    "            ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "        param_lr2 = {'features__univ_select__k':[5,10,20,30],\n",
    "                    'lr__penalty':['l1', 'l2'],\n",
    "                    'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR02:')\n",
    "        cv_outer = LeaveOneOut()\n",
    "        y_true,y_pred = list(),list()\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "            y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            # inner loop for feature selection and hyperparameter tuning \n",
    "            cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "            result = GridSearchCV(pipe_lr2, param_lr2, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "            best_param = result.best_estimator_\n",
    "            # evaluate model on the hold out validation set\n",
    "            yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "            y_pred.append(yhat_proba)\n",
    "            y_true.append(list(y_test[0]))\n",
    "            # report progress\n",
    "            print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "           # Calculate roc_auc on the hold out dataset\n",
    "        print('auc: %.3f' % roc_auc_score(y_true, y_pred))       \n",
    "        print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Logistic Regression 1 3 \n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202]\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "        # prepare data (log transform)\n",
    "        from scipy.stats import skew\n",
    "        skewness = X.apply(lambda x: skew(x))\n",
    "        skewed_feats = skewness[skewness > 0.75]\n",
    "        skewed_feats = skewed_feats.index\n",
    "        minimum = X[skewed_feats].min(axis = 1).min()\n",
    "        skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "        X = X.to_numpy()\n",
    "        \n",
    "        # Logistic Regression 01 (log transform, Scaler, RFECV, LR)\n",
    "        pipe01 = Pipeline([('scaler',MinMaxScaler()),\n",
    "                         ('features',RFECV(estimator = LogisticRegression(solver='liblinear'),cv =3, scoring ='roc_auc')),\n",
    "                         ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "        param01 = {'lr__penalty':['l1','l2'],\n",
    "                'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR01:')\n",
    "        self_LR_pipe(X,y,pipe01,param01)\n",
    "        print('*'*30)\n",
    "        \n",
    "        # Logistic Regression 03 (log transform, scaler, SelectFromModel, LR)\n",
    "        selector = SelectFromModel(estimator=LogisticRegression(solver = 'liblinear'))\n",
    "        pipe_lr03 = Pipeline([ \n",
    "                ('scaler',MinMaxScaler()),('features',selector),\n",
    "            ('lr',LogisticRegression(solver = 'liblinear',random_state=42))]) \n",
    "        param_lr03 = {'features__max_features':[20,30,40],\n",
    "                        'lr__penalty':['l1', 'l2'],\n",
    "                        'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR03:')               \n",
    "        self_LR_pipe(X,y,pipe=pipe_lr03,param=param_lr03)\n",
    "        print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Random Forest01\n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202].to_numpy()\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "        # Random forest 01 (scaler, pca + mututal info classifier, RandomForest)\n",
    "        pca = PCA(n_components = 0.9) \n",
    "        selection = SelectKBest(mutual_info_classif,k=10)\n",
    "        combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "        pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                         ('features',combined_features),\n",
    "                         ('rf',RandomForestClassifier(random_state=42))]) \n",
    "        param = {'features__univ_select__k':[10,20,30,40],\n",
    "                 'rf__max_features':['sqrt','log2'],\n",
    "                 'rf__n_estimators':[50,100,1000,2000]} \n",
    "        print('*'*50)\n",
    "        print('Below is the results from the RF01:')\n",
    "        cv_outer = LeaveOneOut()\n",
    "        y_true,y_pred = list(),list()\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "            y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            ## inner loop for feature selection and hyperparameter tuning \n",
    "            cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "            result = GridSearchCV(pipe, param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "            best_param = result.best_estimator_\n",
    "            # evaluate model on the hold out evaluation dataset\n",
    "            yhat_proba = list(best_param.predict_proba(X_test)[:,1])# reture the probability of predicting '1'\n",
    "            y_pred.append(yhat_proba)\n",
    "            y_true.append(list(y_test[0]))\n",
    "            # report progress\n",
    "            print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "            # Calculate roc_auc on the hold out dataset\n",
    "        print('auc: %.3f' % roc_auc_score(y_true, y_pred))\n",
    "        print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CN and AU Random Forest 2\n",
    "# df_cancer ={}\n",
    "# AUC = dict()\n",
    "# ACU = dict()\n",
    "# SEN = dict()\n",
    "# SPE = dict()\n",
    "# for index, rad in enumerate(rads):\n",
    "#     for key,value in Side_View.items():\n",
    "#         cancer_sort = value.sort_values(by = rad)\n",
    "#         cancer_sort_copy = cancer_sort.copy()\n",
    "#         cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[1,3,0])\n",
    "#         cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "#         print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "#         X = cancer_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "#         y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "#         over = SMOTE(sampling_strategy=\"minority\",random_state=2)\n",
    "#         X,y = over.fit_resample(X,y)\n",
    "\n",
    "\n",
    "        \n",
    "#         # Random Forest 02 (scaler, SelectFromModel, RandomForest)\n",
    "#         selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "#         pipe = Pipeline([ #('scaler',MinMaxScaler()),\n",
    "#                          ('features',selector),\n",
    "#                          ('rf',RandomForestClassifier(random_state=42))]) \n",
    "#         param = {'features__max_features':[5,10,15],##################\n",
    "#                          'rf__max_features':['sqrt','log2'], \n",
    "#                      'rf__n_estimators':[50,100,1000,2000] } \n",
    "#         print('*'*50)\n",
    "#         print('Below is the results from the RF02:')\n",
    "#         Dict,AUC_score,Accuracy,sensitivity1,specificity1  = self_rf_pipe(X,y,pipe,param)  \n",
    "#         print('*'*50)\n",
    "#         df_cancer[key]=Dict    \n",
    "#         AUC[key]=AUC_score\n",
    "#         ACU[key]=Accuracy\n",
    "#         SEN[key]=sensitivity1\n",
    "#         SPE[key]=specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS Case_based_CC_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.824, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.787, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.741, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.806, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.824, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.944, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.907, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.907, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.907, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.856, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.866, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.870, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.866, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.731, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.787, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      "Specificity :  0.5714285714285714\n",
      "The AUC for this view is : 0.6696428571428571\n",
      "The ACC for this view is : 0.6\n",
      "The sen for this view is : 0.625\n",
      "The spe for this view is : 0.5714285714285714\n",
      "ThIS IS Case_based_MLO_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.588, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.736, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.685, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.620, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.583, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.833, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.583, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.556, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.745, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.731, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.731, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.759, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.727, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.704, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.741, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      "Specificity :  0.7142857142857143\n",
      "The AUC for this view is : 0.6071428571428571\n",
      "The ACC for this view is : 0.6666666666666666\n",
      "The sen for this view is : 0.625\n",
      "The spe for this view is : 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "# FOR CHINENSE READERS\n",
    "rad = rads[0]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    cancer_sort = value.sort_values(by = rad)\n",
    "    cancer_sort_copy = cancer_sort.copy()\n",
    "    cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[0,3,1])\n",
    "    cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==0],cancer_sort_copy[cancer_sort_copy['percentile']==1]],axis = 0)\n",
    "\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = cancer_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "    y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "    pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,15],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    \n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.7333333333333333\n",
      "Sensitivity is : 0.5\n",
      "Specificity is : 1.0\n",
      "AUC for normal pipeline for Australian readers is 0.714\n",
      "Confidence interval for the score: [0.409 - 0.98]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(2,-1)\n",
    "\n",
    "# get the maximum prob among predictions of the four views\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Australian readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS Case_based_CC_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.574, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.593, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.722, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.685, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.667, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.741, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.611, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.787, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.593, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.694, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.648, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.528, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.639, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.681, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      "Specificity :  0.16666666666666666\n",
      "The AUC for this view is : 0.5\n",
      "The ACC for this view is : 0.5\n",
      "The sen for this view is : 0.7\n",
      "The spe for this view is : 0.16666666666666666\n",
      "ThIS IS Case_based_MLO_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.685, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.602, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.602, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.759, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.704, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.685, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.722, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.796, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.676, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.750, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.644, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.685, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.630, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.824, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.574, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.602, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.5\n",
      "The AUC for this view is : 0.5583333333333333\n",
      "The ACC for this view is : 0.6875\n",
      "The sen for this view is : 0.8\n",
      "The spe for this view is : 0.5\n"
     ]
    }
   ],
   "source": [
    "# FOR AUSTRALIAN READERS\n",
    "rad = rads[1]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    cancer_sort = value.sort_values(by = rad)\n",
    "    cancer_sort_copy = cancer_sort.copy()\n",
    "    cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[0,3,1])\n",
    "    cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==0],cancer_sort_copy[cancer_sort_copy['percentile']==1]],axis = 0)\n",
    "\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = cancer_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "    y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "    pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,15],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    \n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.5625\n",
      "Sensitivity is : 0.6\n",
      "Specificity is : 0.5\n",
      "AUC for normal pipeline for Australian readers is 0.417\n",
      "Confidence interval for the score: [0.077 - 0.792]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(2,-1)\n",
    "\n",
    "# get the maximum prob among predictions of the four views\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Australian readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Based\n",
    "### Cancer Lesion Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_feat_location = pd.read_csv(\"/Users/jessy/Desktop/笔记本/Radiomics Data Analysis/diffScore w_out 10 dep Cn/case_feat_location_diffScore/Square_feat.csv\")\n",
    "cancer_location = case_feat_location.merge(lesion_side.rename(columns={\"LesionNumber\":\"LesionNum\"}), how='left',on = [\"CaseName\",\"LesionNum\"])\n",
    "CC_CL_LC = cancer_location.loc[cancer_location[\"Side\"]==cancer_location[\"LesionSide\"],:].loc[cancer_location[\"View\"]==\"'CC'\",:]\n",
    "MLO_CL_LC = cancer_location.loc[cancer_location[\"Side\"]==cancer_location[\"LesionSide\"],:].loc[cancer_location[\"View\"]==\"'MLO'\",:]\n",
    "\n",
    "CC_CL_LC = CC_CL_LC[CC_CL_LC.LesionNum!=2]\n",
    "MLO_CL_LC = MLO_CL_LC[MLO_CL_LC.LesionNum!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer lesion side \n",
    "Side_View = {}\n",
    "Side_View = {'Loc_based_CC_CancerLesion':CC_CL_LC,\n",
    "             'Loc_based_MLO_CancerLesion': MLO_CL_LC}\n",
    "rads = ['diffScore.CN','diffScore.AU']\n",
    "#Bins = [[0,0.2,0.58,1],[0,0.5,0.77,1]] \n",
    "# For location \n",
    "# Chinese [0,0,25,0.7,1]\n",
    "# Au [0,0.5625,0.7500,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Logistic Regression 02\n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202]\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "        # prepare data (log transform)\n",
    "        from scipy.stats import skew\n",
    "        skewness = X.apply(lambda x: skew(x))\n",
    "        skewed_feats = skewness[skewness > 0.75]\n",
    "        skewed_feats = skewed_feats.index\n",
    "        minimum = X[skewed_feats].min(axis = 1).min()\n",
    "        skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "        X = X.to_numpy()  \n",
    "\n",
    "        pipe_lr2 = Pipeline([\n",
    "            ('scaler',MinMaxScaler()),\n",
    "            ('features',combined_features),\n",
    "            ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "        param_lr2 = {'features__univ_select__k':[5,10,20,30],\n",
    "                    'lr__penalty':['l1', 'l2'],\n",
    "                    'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR02:')\n",
    "        cv_outer = LeaveOneOut()\n",
    "        y_true,y_pred = list(),list()\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "            y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            # inner loop for feature selection and hyperparameter tuning \n",
    "            cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "            result = GridSearchCV(pipe_lr2, param_lr2, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "            best_param = result.best_estimator_\n",
    "            # evaluate model on the hold out validation set\n",
    "            yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "            y_pred.append(yhat_proba)\n",
    "            y_true.append(list(y_test[0]))\n",
    "            # report progress\n",
    "            print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "           # Calculate roc_auc on the hold out dataset\n",
    "        print('auc: %.3f' % roc_auc_score(y_true, y_pred))       \n",
    "        print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Logistic Regression 1 3 \n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202]\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "        # prepare data (log transform)\n",
    "        from scipy.stats import skew\n",
    "        skewness = X.apply(lambda x: skew(x))\n",
    "        skewed_feats = skewness[skewness > 0.75]\n",
    "        skewed_feats = skewed_feats.index\n",
    "        minimum = X[skewed_feats].min(axis = 1).min()\n",
    "        skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "        X = X.to_numpy()\n",
    "        \n",
    "        # Logistic Regression 01 (log transform, Scaler, RFECV, LR)\n",
    "        pipe01 = Pipeline([('scaler',MinMaxScaler()),\n",
    "                         ('features',RFECV(estimator = LogisticRegression(solver='liblinear'),cv =3, scoring ='roc_auc')),\n",
    "                         ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "        param01 = {'lr__penalty':['l1','l2'],\n",
    "                'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR01:')\n",
    "        self_LR_pipe(X,y,pipe01,param01)\n",
    "        print('*'*30)\n",
    "        \n",
    "        # Logistic Regression 03 (log transform, scaler, SelectFromModel, LR)\n",
    "        selector = SelectFromModel(estimator=LogisticRegression(solver = 'liblinear'))\n",
    "        pipe_lr03 = Pipeline([ \n",
    "                ('scaler',MinMaxScaler()),('features',selector),\n",
    "            ('lr',LogisticRegression(solver = 'liblinear',random_state=42))]) \n",
    "        param_lr03 = {'features__max_features':[20,30,40],\n",
    "                        'lr__penalty':['l1', 'l2'],\n",
    "                        'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR03:')               \n",
    "        self_LR_pipe(X,y,pipe=pipe_lr03,param=param_lr03)\n",
    "        print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Random Forest01\n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202].to_numpy()\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "        \n",
    "        # Random forest 01 (scaler, pca + mututal info classifier, RandomForest)\n",
    "        pca = PCA(n_components = 0.9) \n",
    "        selection = SelectKBest(mutual_info_classif,k=10)\n",
    "        combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "        pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                         ('features',combined_features),\n",
    "                         ('rf',RandomForestClassifier(random_state=42))]) \n",
    "        param = {'features__univ_select__k':[10,20,30,40],\n",
    "                 'rf__max_features':['sqrt','log2'],\n",
    "                 'rf__n_estimators':[50,100,1000,2000]} \n",
    "        print('*'*50)\n",
    "        print('Below is the results from the RF01:')\n",
    "        cv_outer = LeaveOneOut()\n",
    "        y_true,y_pred = list(),list()\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "            y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            ## inner loop for feature selection and hyperparameter tuning \n",
    "            cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "            result = GridSearchCV(pipe, param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "            best_param = result.best_estimator_\n",
    "            # evaluate model on the hold out evaluation dataset\n",
    "            yhat_proba = list(best_param.predict_proba(X_test)[:,1])# reture the probability of predicting '1'\n",
    "            y_pred.append(yhat_proba)\n",
    "            y_true.append(list(y_test[0]))\n",
    "            # report progress\n",
    "            print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "            # Calculate roc_auc on the hold out dataset\n",
    "        print('auc: %.3f' % roc_auc_score(y_true, y_pred))\n",
    "        print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CN and AU Random Forest 2\n",
    "# df_location = {}\n",
    "# AUC = dict()\n",
    "# ACU = dict()\n",
    "# SEN = dict()\n",
    "# SPE = dict()\n",
    "# for index, rad in enumerate(rads):\n",
    "#     for key,value in Side_View.items():\n",
    "#         cancer_sort = value.sort_values(by = rad)\n",
    "#         cancer_sort_copy = cancer_sort.copy()\n",
    "#         cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[1,3,0])\n",
    "#         cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "#         print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "#         X = cancer_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "#         y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "#         over = SMOTE(sampling_strategy=\"minority\",random_state=2)\n",
    "#         X,y = over.fit_resample(X,y)\n",
    "\n",
    "\n",
    "#         # Random Forest 02 (scaler, SelectFromModel, RandomForest)\n",
    "#         selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "#         pipe = Pipeline([ #('scaler',MinMaxScaler()),\n",
    "#                          ('features',selector),\n",
    "#                          ('rf',RandomForestClassifier(random_state=42))]) \n",
    "#         param = {'features__max_features':[5,10,15],\n",
    "#                          'rf__max_features':['sqrt','log2'], \n",
    "#                      'rf__n_estimators':[50,100,1000,2000] } \n",
    "#         print('*'*50)\n",
    "#         print('Below is the results from the RF02:')\n",
    "#         Dict,AUC_score,Accuracy,sensitivity1,specificity1  = self_rf_pipe(X,y,pipe,param)  \n",
    "#         print('*'*50) \n",
    "#         df_location[key]=Dict\n",
    "#         AUC[key]=AUC_score\n",
    "#         ACU[key]=Accuracy\n",
    "#         SEN[key]=sensitivity1\n",
    "#         SPE[key]=specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS Loc_based_CC_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.630, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.620, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.583, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.556, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.602, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.667, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.704, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.926, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.778, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.657, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.708, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.819, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      "Specificity :  0.14285714285714285\n",
      "The AUC for this view is : 0.16326530612244897\n",
      "The ACC for this view is : 0.2857142857142857\n",
      "The sen for this view is : 0.42857142857142855\n",
      "The spe for this view is : 0.14285714285714285\n",
      "ThIS IS Loc_based_MLO_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.514, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.662, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.551, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      "Specificity :  0.0\n",
      "The AUC for this view is : 0.0\n",
      "The ACC for this view is : 0.35714285714285715\n",
      "The sen for this view is : 0.7142857142857143\n",
      "The spe for this view is : 0.0\n"
     ]
    }
   ],
   "source": [
    "# FOR CHINENSE READERS\n",
    "rad = rads[0]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    cancer_sort = value.sort_values(by = rad)\n",
    "    cancer_sort_copy = cancer_sort.copy()\n",
    "    cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[0,3,1])\n",
    "    cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = cancer_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "    y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "    pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,15],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    \n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.21428571428571427\n",
      "Sensitivity is : 0.2857142857142857\n",
      "Specificity is : 0.14285714285714285\n",
      "AUC for normal pipeline for Chinese readers is 0.122\n",
      "Confidence interval for the score: [0.000 - 0.375]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(2,-1)\n",
    "# get the maximum prob among predictions of the four views\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Chinese readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS Loc_based_CC_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.519, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.537, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.537, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.523, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.519, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      "Specificity :  0.16666666666666666\n",
      "The AUC for this view is : 0.12962962962962962\n",
      "The ACC for this view is : 0.5333333333333333\n",
      "The sen for this view is : 0.7777777777777778\n",
      "The spe for this view is : 0.16666666666666666\n",
      "ThIS IS Loc_based_MLO_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.722, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.611, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.648, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.667, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.574, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.565, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.546, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.556, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.602, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.519, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.16666666666666666\n",
      "The AUC for this view is : 0.3148148148148149\n",
      "The ACC for this view is : 0.3333333333333333\n",
      "The sen for this view is : 0.4444444444444444\n",
      "The spe for this view is : 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "# FOR AU READERS\n",
    "rad = rads[1]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    cancer_sort = value.sort_values(by = rad)\n",
    "    cancer_sort_copy = cancer_sort.copy()\n",
    "    cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[0,3,1])\n",
    "    cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = cancer_sort_drop.loc[:,\"feat.Square.1\":\"feat.Square.203\"].to_numpy()\n",
    "    y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "    pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,15],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    \n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "#     PRED.append(pre_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.3333333333333333\n",
      "Sensitivity is : 0.3333333333333333\n",
      "Specificity is : 0.3333333333333333\n",
      "AUC for normal pipeline for Chinese readers is 0.259\n",
      "Confidence interval for the score: [0.000 - 0.577]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(2,-1)\n",
    "# get the maximum prob among predictions of the four views\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Chinese readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
