{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import RFECV,SelectKBest,mutual_info_classif,SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV,LeaveOneOut,RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,auc,confusion_matrix\n",
    "from mat4py import loadmat\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether there is missing value in normal mat: False\n",
      "Whether there is missing value in cancer mat: False\n"
     ]
    }
   ],
   "source": [
    "# Case-based difficuty score\n",
    "case_feat_normal = pd.read_csv('/Users/jessy/Desktop/笔记本/Radiomics Data Analysis/diffScore w_out 10 dep Cn/case_feat_diffScore/whole_feat.csv')\n",
    "case_feat_normal = pd.DataFrame(case_feat_normal).drop(\"Unnamed: 0\",axis=1)\n",
    "case_feat_normal.drop([120,121,122,123], axis=0, inplace=True)\n",
    "print('Whether there is missing value in normal mat: {}'.format(case_feat_normal.isnull().values.any())) \n",
    "case_feat_cancer = pd.read_csv('/Users/jessy/Desktop/笔记本/Radiomics Data Analysis/diffScore w_out 10 dep Cn/case_feat_case_cancer_diffScore/whole_feat.csv')\n",
    "case_feat_cancer = pd.DataFrame(case_feat_cancer).drop(\"Unnamed: 0\",axis=1)\n",
    "print('Whether there is missing value in cancer mat: {}'.format(case_feat_cancer.isnull().values.any())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of CC_Normal is (78, 210)\n",
      "The shape of CC_CancerLesion is (20, 212)\n",
      "The shape of MLO_Normal is (78, 210)\n",
      "The shape of MLO_CancerLesion is (20, 212)\n"
     ]
    }
   ],
   "source": [
    "CC_Normal = case_feat_normal.loc[case_feat_normal['View']==\"'CC'\",]\n",
    "MLO_Normal = case_feat_normal.loc[case_feat_normal['View']==\"'MLO'\",]\n",
    "left_CC = CC_Normal.loc[CC_Normal[\"Side\"]==\"L\",]\n",
    "right_CC =CC_Normal.loc[CC_Normal[\"Side\"]==\"R\",]\n",
    "left_MLO = MLO_Normal.loc[MLO_Normal[\"Side\"]==\"L\",]\n",
    "right_MLO = MLO_Normal.loc[MLO_Normal[\"Side\"]==\"R\",]\n",
    "lesion_side = pd.read_csv(\"/Users/jessy/Desktop/笔记本/Radiomics Data Analysis/diffScore w_out 10 dep Cn/lesion_side.csv\")\n",
    "case_feat_cancer_ = case_feat_cancer.merge(lesion_side, how='left',on = \"CaseName\").drop(\"Unnamed: 0\",axis=1)\n",
    "CC_Cancer = case_feat_cancer_.loc[case_feat_cancer_[\"Side\"]==case_feat_cancer_[\"LesionSide\"] ,].loc[case_feat_cancer_[\"View\"]==\"'CC'\",]\n",
    "MLO_Cancer = case_feat_cancer_.loc[case_feat_cancer_[\"Side\"]==case_feat_cancer_[\"LesionSide\"] ,].loc[case_feat_cancer_[\"View\"]==\"'MLO'\",]\n",
    "\n",
    "CC_Cancer.drop(17, axis=0, inplace=True)\n",
    "MLO_Cancer.drop(23, axis=0, inplace=True)\n",
    "print('The shape of CC_Normal is {}'.format(CC_Normal.shape))\n",
    "print('The shape of CC_CancerLesion is {}'.format(CC_Cancer.shape))\n",
    "print('The shape of MLO_Normal is {}'.format(MLO_Normal.shape))\n",
    "print('The shape of MLO_CancerLesion is {}'.format(MLO_Cancer.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algorithms to use: Logistic regression\n",
    "def self_LR_pipe(X,y,pipe, param):\n",
    "    random.seed(10)\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # inner loop for feature selection and hyperparameter tuning \n",
    "        pipe_lr = pipe\n",
    "        param_grid = param\n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        result = GridSearchCV(pipe_lr, param_grid=param, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out validation set\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "        print('Selected features are: {}'.format(np.where(result.best_estimator_.named_steps['features'].get_support())[0]))\n",
    "    \n",
    "    # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithms to use: Random Forest\n",
    "def self_rf_pipe(X,y,pipe, param):\n",
    "    features = {}\n",
    "    row = 0\n",
    "    random.seed(24)\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred,Predicted_class  = list(),list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        row+=1\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        ## inner loop for feature selection and hyperparameter tuning \n",
    "\n",
    "        cv_inner = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=24)\n",
    "        result = GridSearchCV(pipe, param_grid=param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out evaluation dataset\n",
    "        \n",
    "        yhat_proba = best_param.predict_proba(X_test)[:,1]# reture the probability of predicting '1'\n",
    "        y_pred.append(yhat_proba[0])\n",
    "        Predicted_class.append(best_param.predict(X_test))\n",
    "        y_true.append(y_test[0])\n",
    "        # report progress\n",
    "        #print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "        features[row] = np.where(result.best_estimator_.named_steps['features'].get_support())[0]\n",
    "\n",
    "    \n",
    "    Dict = {}\n",
    "    for key in sorted(features):\n",
    "        for num in features[key]:\n",
    "            if num not in Dict:\n",
    "                Dict[num] = 1\n",
    "            else:\n",
    "                Dict[num]+=1\n",
    "    \n",
    "    KEY = []\n",
    "\n",
    "    for key in sorted(Dict):\n",
    "        #print(key,':',Dict[key])\n",
    "        if Dict[key] >= len(X)/2:\n",
    "            KEY.append(key)\n",
    "    print(KEY) \n",
    "\n",
    "    # Calculate roc_auc on the hold out dataset\n",
    "    AUC_score = roc_auc_score(y_true, y_pred)\n",
    "    Accuracy = accuracy_score(y_true,Predicted_class)\n",
    "    print('auc: %.3f' % AUC_score)\n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "   \n",
    "    cm1 = confusion_matrix(y_true,Predicted_class)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    \n",
    "    # bootstrap auc\n",
    "    n_bootstraps = 2000\n",
    "    rng_seed = 42  # control reproducibility\n",
    "    bootstrapped_scores = []\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(y_pred), len(y_pred))\n",
    "        if len(np.array(y_true)[indices.astype(int)]) < 2:\n",
    "            continue\n",
    "\n",
    "        score = roc_auc_score(np.array(y_true)[indices.astype(int)], np.array(y_pred)[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "        #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    # Computing the lower and upper bound of the 90% confidence interval\n",
    "    # You can change the bounds percentiles to 0.025 and 0.975 to get\n",
    "    # a 95% confidence interval instead.\n",
    "    confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "    print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(confidence_lower, confidence_upper))\n",
    "    \n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "\n",
    "    return AUC_score,Accuracy,sensitivity1,specificity1, sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM: to evaluate the entire pipeline\n",
    "def pipeline_evaluate(X,y,pipe, param):\n",
    "    random.seed(24)\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred, Predicted_class  = list(),list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        ## inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=2)\n",
    "        result = GridSearchCV(pipe, param_grid=param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out evaluation dataset\n",
    "        yhat_proba = best_param.predict_proba(X_test)[:,1]# reture the probability of predicting '1'\n",
    "        y_pred.append(yhat_proba[0])\n",
    "        Predicted_class.append(best_param.predict(X_test))\n",
    "        y_true.append(y_test[0])\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "    return y_pred,Predicted_class, y_true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Side_View = {}\n",
    "Side_View = {'left_CC':left_CC,\n",
    "             'right_CC':right_CC,\n",
    "             'left_MLO':left_MLO,\n",
    "             'right_MLO':right_MLO}\n",
    "rads = ['diffScore.CN','diffScore.AU']\n",
    "#Bins = [[0, 0.60, 0.8, 1],[0, 0.69, 0.81, 1]] # the left side is not included, right side included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic Regression 02 (log, pca + mutual_info, LR)\n",
    "rad = rads[0]\n",
    "bins = Bins[0]\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202]\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    # prepare data (log transform)\n",
    "    from scipy.stats import skew\n",
    "    skewness = X.apply(lambda x: skew(x))\n",
    "    skewed_feats = skewness[skewness > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    minimum = X[skewed_feats].min(axis = 1).min()\n",
    "    skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    pca = PCA(n_components = 0.9)\n",
    "    selection = SelectKBest(mutual_info_classif,k=10)\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "    pipe_lr2 = Pipeline([\n",
    "        ('scaler',MinMaxScaler()),\n",
    "        ('features',combined_features),\n",
    "        ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "    param_lr2 = {'features__univ_select__k':[5,10,20,30],\n",
    "                'lr__penalty':['l1', 'l2'],\n",
    "                'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "\n",
    "    print('Below is the results from the LR02:')\n",
    "\n",
    "\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        result = GridSearchCV(pipe_lr2, param_lr2, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out validation set\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "       # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Chinese Rads Logistic regression 1 3 \n",
    "print('This is for Chinese Radiologists: \\n')\n",
    "rad = rads[0]\n",
    "bins = Bins[0]\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202]\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    # prepare data (log transform)\n",
    "    from scipy.stats import skew\n",
    "    skewness = X.apply(lambda x: skew(x))\n",
    "    skewed_feats = skewness[skewness > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    minimum = X[skewed_feats].min(axis = 1).min()\n",
    "    skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    # Logistic Regression 01 (log transform, Scaler, RFECV, LR)\n",
    "    pipe01 = Pipeline([('scaler',MinMaxScaler()),\n",
    "                     ('features',RFECV(estimator = LogisticRegression(solver='liblinear'),cv =3, scoring ='roc_auc')),\n",
    "                     ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "    param01 = {'lr__penalty':['l1','l2'],\n",
    "            'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "\n",
    "    print('*'*30)\n",
    "    print('Below is the results from the LR01:')\n",
    "    self_LR_pipe(X,y,pipe01,param01) \n",
    "    print('*'*30)\n",
    "\n",
    "\n",
    "    # Logistic Regression 03 (log transform, scaler, SelectFromModel, LR)\n",
    "    selector = SelectFromModel(estimator=LogisticRegression(solver = 'liblinear'))\n",
    "    pipe_lr03 = Pipeline([ \n",
    "            ('scaler',MinMaxScaler()),\n",
    "            ('features',selector),\n",
    "            ('lr',LogisticRegression(solver = 'liblinear',random_state=42))]) \n",
    "    param_lr03 = {'features__max_features':[20,30,40],\n",
    "                    'lr__penalty':['l1', 'l2'],\n",
    "                    'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "    print('*'*30)\n",
    "    print('Below is the results from the LR03:') \n",
    "    self_LR_pipe(X,y,pipe=pipe_lr03,param=param_lr03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Chinese Rads Random Forest 01\n",
    "rad = rads[0]\n",
    "bins = Bins[0]\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202].to_numpy()\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    # Random forest 01 (scaler, pca + mututal info classifier, RandomForest)\n",
    "    pca = PCA(n_components = 0.9) \n",
    "    selection = SelectKBest(mutual_info_classif,k=10)\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "    pipe = Pipeline([('scaler',MinMaxScaler()),\n",
    "                    ('features',combined_features),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))])\n",
    "\n",
    "    param = {'features__univ_select__k':[10,20,30,40],\n",
    "             'rf__max_features':['sqrt','log2'],\n",
    "             'rf__n_estimators':[50,100,1000,2000]} \n",
    "    print('*'*50)\n",
    "    print('Below is the results from the RF01:')\n",
    "\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        ## inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        result = GridSearchCV(pipe, param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out evaluation dataset\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])# reture the probability of predicting '1'\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "        # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 02 (scaler, SelectFromModel, RandomForest)\n",
    "# rad = rads[0]\n",
    "# #bins = Bins[0]\n",
    "# for key,value in Side_View.items():\n",
    "#     Normal_sort = value.sort_values(by = rad)\n",
    "#     Normal_sort_copy = Normal_sort.copy()\n",
    "#     Normal_sort_copy['percentile']=pd.qcut(Normal_sort_copy[rad],3, labels=[1,3,0])\n",
    "#     Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "#     print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "#     X = Normal_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "#     y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "# #     over = SMOTE(sampling_strategy=\"minority\",random_state=2)\n",
    "# #     X,y = over.fit_resample(X,y)  \n",
    "\n",
    "#     selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 5)\n",
    "#     pipe = Pipeline([ #('scaler',MinMaxScaler()),\n",
    "#                      ('features',selector),\n",
    "#                      ('rf',RandomForestClassifier(random_state=42))]) \n",
    "#     param = {'features__max_features':[5,10,20],\n",
    "#             \"rf__max_depth\":[1,5,10],\n",
    "#              \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "#                  'rf__n_estimators':[50,100,1000] } \n",
    "#     print('*'*50)\n",
    "#     AUC_score,Accuracy,sensitivity1,specificity1, sorted_scores = self_rf_pipe(X,y,pipe,param) \n",
    "#     print(sorted_scores)\n",
    "#     print('*'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS left_CC SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.778, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.808, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.840, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.806, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.810, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.890, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.847, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.876, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.890, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.812, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.795, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.819, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.834, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.850, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.877, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.935, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.868, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.874, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.857, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.857, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.880, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.869, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.863, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.854, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.844, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.847, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      "Specificity :  0.5454545454545454\n",
      "The AUC for this view is : 0.7787878787878788\n",
      "The ACC for this view is : 0.6923076923076923\n",
      "The sen for this view is : 0.8\n",
      "The spe for this view is : 0.5454545454545454\n",
      "ThIS IS right_CC SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.724, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.768, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.743, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.741, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.705, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.762, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.726, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.802, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.823, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.713, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.759, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.748, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.747, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.748, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.802, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.828, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.732, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.768, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.749, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.767, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.801, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.811, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.825, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.789, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.775, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.787, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.18181818181818182\n",
      "The AUC for this view is : 0.5303030303030303\n",
      "The ACC for this view is : 0.46153846153846156\n",
      "The sen for this view is : 0.6666666666666666\n",
      "The spe for this view is : 0.18181818181818182\n",
      "ThIS IS left_MLO SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.797, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.831, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.819, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.869, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.850, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.850, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.849, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.850, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.833, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.830, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.866, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.819, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.810, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.832, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.834, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">est=0.835, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.870, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.806, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.801, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.785, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.852, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.802, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.793, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.771, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.783, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.894, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      "Specificity :  0.6363636363636364\n",
      "The AUC for this view is : 0.7515151515151515\n",
      "The ACC for this view is : 0.6923076923076923\n",
      "The sen for this view is : 0.7333333333333333\n",
      "The spe for this view is : 0.6363636363636364\n",
      "ThIS IS right_MLO SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.930, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.970, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.942, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.945, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.941, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.969, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.946, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.954, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.959, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.941, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.930, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.952, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.936, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.961, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.989, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.976, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.963, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.969, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.961, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.957, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.970, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.958, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.952, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.950, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.986, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.910, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      "Specificity :  0.8181818181818182\n",
      "The AUC for this view is : 0.9181818181818182\n",
      "The ACC for this view is : 0.8461538461538461\n",
      "The sen for this view is : 0.8666666666666667\n",
      "The spe for this view is : 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE EVALUATE FOR CHINESE \n",
    "rad = rads[0]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.qcut(Normal_sort_copy[rad],3, labels=[0,3,1])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==0],Normal_sort_copy[Normal_sort_copy['percentile']==1]],axis = 0)\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 5)\n",
    "    pipe = Pipeline([('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,20],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6538461538461539"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy = accuracy_score(true_value,label)\n",
    "Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.6538461538461539\n",
      "Sensitivity is : 0.4\n",
      "Specificity is : 1.0\n",
      "AUC for normal pipeline for Chinese readers is 0.900\n",
      "Confidence interval for the score: [0.758 - 0.997]\n"
     ]
    }
   ],
   "source": [
    "pipeline_prob_cn = np.array(PROB).reshape(4,-1)\n",
    "agg_prob_cn = np.max(pipeline_prob_cn, axis=0).tolist()\n",
    "\n",
    "label = []\n",
    "for i in range(len(agg_prob_cn)):\n",
    "    if agg_prob_cn[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, agg_prob_cn)\n",
    "print(\"AUC for normal pipeline for Chinese readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(agg_prob_cn), len(agg_prob_cn))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], np.array(agg_prob_cn)[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Australian Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic Regression 02 (log, pca + mutual_info, LR) \n",
    "rad = rads[1]\n",
    "bins = Bins[1]\n",
    "print('Below is for Australian Radiologists: \\n')\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202]\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "    # prepare data (log transform)\n",
    "    from scipy.stats import skew\n",
    "    skewness = X.apply(lambda x: skew(x))\n",
    "    skewed_feats = skewness[skewness > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    minimum = X[skewed_feats].min(axis = 1).min()\n",
    "    skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "    X = X.to_numpy()\n",
    "    pca = PCA(n_components = 0.9)\n",
    "    selection = SelectKBest(mutual_info_classif,k=10)\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "    pipe_lr2 = Pipeline([\n",
    "        ('scaler',MinMaxScaler()),\n",
    "        ('features',combined_features),\n",
    "        ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "    param_lr2 = {'features__univ_select__k':[5,10,20,30],\n",
    "                'lr__penalty':['l1', 'l2'],\n",
    "                'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "\n",
    "    print('Below is the results from the LR02:')\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        result = GridSearchCV(pipe_lr2, param_lr2, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out validation set\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "       # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))       \n",
    "\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Australian Rads Logistic regression 1 3\n",
    "rad = rads[1]\n",
    "bins = Bins[1]\n",
    "print('Below is for Australian Radiologists: \\n')\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202]\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "    # prepare data (log transform)\n",
    "    from scipy.stats import skew\n",
    "    skewness = X.apply(lambda x: skew(x))\n",
    "    skewed_feats = skewness[skewness > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    minimum = X[skewed_feats].min(axis = 1).min()\n",
    "    skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    # Logistic Regression 01 (log transform, Scaler, RFECV, LR)\n",
    "    pipe01 = Pipeline([('scaler',MinMaxScaler()),\n",
    "                     ('features',RFECV(estimator = LogisticRegression(solver='liblinear'),cv =3, scoring ='roc_auc')),\n",
    "                     ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "    param01 = {'lr__penalty':['l1','l2'],\n",
    "            'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "    print('*'*30)\n",
    "    print('Below is the results from the LR01:')\n",
    "    self_LR_pipe(X,y,pipe01,param01)\n",
    "    print('*'*30)\n",
    "\n",
    "\n",
    "    # Logistic Regression 03 (log transform, scaler, SelectFromModel, LR)\n",
    "    selector = SelectFromModel(estimator=LogisticRegression(solver = 'liblinear'))\n",
    "    pipe_lr03 = Pipeline([ \n",
    "            ('scaler',MinMaxScaler()),\n",
    "        ('features',selector),\n",
    "        ('lr',LogisticRegression(solver = 'liblinear',random_state=42))]) \n",
    "    param_lr03 = {'features__max_features':[20,30,40],\n",
    "                    'lr__penalty':['l1', 'l2'],\n",
    "                    'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "    print('*'*30)\n",
    "    print('Below is the results from the LR03:')               \n",
    "    self_LR_pipe(X,y,pipe=pipe_lr03,param=param_lr03)\n",
    "    print('*'*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Australian Rads Random Forest 01\n",
    "rad = rads[1]\n",
    "bins = Bins[1]\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.cut(Normal_sort_copy[rad],bins, labels=[1,3,0])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,0:202].to_numpy()\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "    # Random forest 01 (scaler, pca + mututal info classifier, RandomForest)\n",
    "    pca = PCA(n_components = 0.9) \n",
    "    selection = SelectKBest(mutual_info_classif,k=10)\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "    pipe = Pipeline([('scaler',MinMaxScaler()),\n",
    "                    ('features',combined_features),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))])\n",
    "\n",
    "    param = {'features__univ_select__k':[10,20,30,40],\n",
    "             'rf__max_features':['sqrt','log2'],\n",
    "             'rf__n_estimators':[50,100,1000,2000]} \n",
    "\n",
    "\n",
    "    print('*'*50)\n",
    "    print('Below is the results from the RF01:')\n",
    "    cv_outer = LeaveOneOut()\n",
    "    y_true,y_pred = list(),list()\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        ## inner loop for feature selection and hyperparameter tuning \n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        result = GridSearchCV(pipe, param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "        best_param = result.best_estimator_\n",
    "        # evaluate model on the hold out evaluation dataset\n",
    "        yhat_proba = list(best_param.predict_proba(X_test)[:,1])# reture the probability of predicting '1'\n",
    "        y_pred.append(yhat_proba)\n",
    "        y_true.append(list(y_test[0]))\n",
    "        # report progress\n",
    "        print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "        # Calculate roc_auc on the hold out dataset\n",
    "    print('auc: %.3f' % roc_auc_score(y_true, y_pred))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Random Forest 02 (scaler, SelectFromModel, RandomForest)\n",
    "# rad = rads[1]\n",
    "# df_au = dict()\n",
    "# AUC = dict()\n",
    "# ACU = dict()\n",
    "# SEN = dict()\n",
    "# SPE = dict()\n",
    "# #bins = Bins[0]\n",
    "# for key,value in Side_View.items():\n",
    "#     Normal_sort = value.sort_values(by = rad)\n",
    "#     Normal_sort_copy = Normal_sort.copy()\n",
    "#     Normal_sort_copy['percentile']=pd.qcut(Normal_sort_copy[rad],3, labels=[1,3,0])\n",
    "#     Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==1],Normal_sort_copy[Normal_sort_copy['percentile']==0]],axis = 0)\n",
    "    \n",
    "#     print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "#     X = Normal_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "#     y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "#     over = SMOTE(sampling_strategy=\"minority\",random_state=2)\n",
    "#     X,y = over.fit_resample(X,y)\n",
    "    \n",
    "\n",
    "#     selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 5)\n",
    "#     pipe = Pipeline([ #('scaler',MinMaxScaler()),\n",
    "#                      ('features',selector),\n",
    "#                      ('rf',RandomForestClassifier(random_state=42))]) \n",
    "#     param = {'features__max_features':[5,10,20],\n",
    "#             \"rf__max_depth\":[1,5,10],\n",
    "#              \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "#                  'rf__n_estimators':[50,100,1000] } \n",
    "#     print('*'*50)\n",
    "#     Dict,AUC_score,Accuracy,sensitivity1,specificity1 = self_rf_pipe(X,y,pipe,param)  \n",
    "#     print('*'*50)\n",
    "#     df_au[key]=Dict\n",
    "#     AUC[key]=AUC_score\n",
    "#     ACU[key]=Accuracy\n",
    "#     SEN[key]=sensitivity1\n",
    "#     SPE[key]=specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS left_CC SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.602, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.594, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.565, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.632, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.563, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.668, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.652, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.643, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.694, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.681, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.674, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.712, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.674, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.749, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.677, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.720, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.694, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.682, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.735, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.764, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.657, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.646, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.643, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.780, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.622, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.725, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.783, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.640, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.631, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      "Specificity :  0.2\n",
      "The AUC for this view is : 0.5526315789473685\n",
      "The ACC for this view is : 0.5172413793103449\n",
      "The sen for this view is : 0.6842105263157895\n",
      "The spe for this view is : 0.2\n",
      "ThIS IS right_CC SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.432, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.430, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.427, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.407, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.387, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.383, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.491, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.498, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.497, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.451, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.502, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.471, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.474, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.443, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.546, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.466, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.560, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.481, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.560, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.465, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.476, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.498, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.534, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.460, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.552, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.541, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.442, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.576, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      "Specificity :  0.0\n",
      "The AUC for this view is : 0.12631578947368421\n",
      "The ACC for this view is : 0.41379310344827586\n",
      "The sen for this view is : 0.631578947368421\n",
      "The spe for this view is : 0.0\n",
      "ThIS IS left_MLO SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.610, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.620, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.566, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.619, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.549, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.569, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.563, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.617, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.559, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">est=0.566, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.568, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.557, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.597, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.647, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.633, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.629, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.650, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.657, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.683, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.677, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.620, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.650, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.667, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.672, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.713, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.662, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.734, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.627, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.591, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.0\n",
      "The AUC for this view is : 0.4236842105263158\n",
      "The ACC for this view is : 0.4827586206896552\n",
      "The sen for this view is : 0.7368421052631579\n",
      "The spe for this view is : 0.0\n",
      "ThIS IS right_MLO SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.501, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.543, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.438, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.454, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.434, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.461, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.552, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.547, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.469, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.483, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.557, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.490, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.546, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.499, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.513, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.508, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.519, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.577, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.559, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.647, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.567, cfg={'features__max_features': 20, 'rf__max_depth': 5, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.601, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 100}\n",
      ">est=0.487, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.605, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 1000}\n",
      ">est=0.504, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.501, cfg={'features__max_features': 20, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.591, cfg={'features__max_features': 20, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.418, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.460, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      "Specificity :  0.0\n",
      "The AUC for this view is : 0.43947368421052635\n",
      "The ACC for this view is : 0.6551724137931034\n",
      "The sen for this view is : 1.0\n",
      "The spe for this view is : 0.0\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE EVALUATE FOR AUSTRALIAN \n",
    "rad = rads[1]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    Normal_sort = value.sort_values(by = rad)\n",
    "    Normal_sort_copy = Normal_sort.copy()\n",
    "    Normal_sort_copy['percentile']=pd.qcut(Normal_sort_copy[rad],3, labels=[0,3,1])\n",
    "    Normal_sort_drop = pd.concat([Normal_sort_copy[Normal_sort_copy['percentile']==0],Normal_sort_copy[Normal_sort_copy['percentile']==1]],axis = 0)\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = Normal_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "    y = Normal_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 5)\n",
    "    pipe = Pipeline([('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,20],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.3103448275862069\n",
      "Sensitivity is : 0.3684210526315789\n",
      "Specificity is : 0.2\n",
      "AUC for normal pipeline for Australian readers is 0.345\n",
      "Confidence interval for the score: [0.134 - 0.571]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(4,-1)\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Australian readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Based \n",
    "### Cancer Lesion Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer lesion side  case based\n",
    "Side_View = {}\n",
    "Side_View = {'Case_based_CC_CancerLesion':CC_Cancer,\n",
    "             'Case_based_MLO_CancerLesion':MLO_Cancer}\n",
    "rads = ['diffScore.CN','diffScore.AU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Logistic Regression 02\n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202]\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "        # prepare data (log transform)\n",
    "        from scipy.stats import skew\n",
    "        skewness = X.apply(lambda x: skew(x))\n",
    "        skewed_feats = skewness[skewness > 0.75]\n",
    "        skewed_feats = skewed_feats.index\n",
    "        minimum = X[skewed_feats].min(axis = 1).min()\n",
    "        skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "        X = X.to_numpy()  \n",
    "\n",
    "        pipe_lr2 = Pipeline([\n",
    "            ('scaler',MinMaxScaler()),\n",
    "            ('features',combined_features),\n",
    "            ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "        param_lr2 = {'features__univ_select__k':[5,10,20,30],\n",
    "                    'lr__penalty':['l1', 'l2'],\n",
    "                    'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR02:')\n",
    "        cv_outer = LeaveOneOut()\n",
    "        y_true,y_pred = list(),list()\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "            y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            # inner loop for feature selection and hyperparameter tuning \n",
    "            cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "            result = GridSearchCV(pipe_lr2, param_lr2, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "            best_param = result.best_estimator_\n",
    "            # evaluate model on the hold out validation set\n",
    "            yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "            y_pred.append(yhat_proba)\n",
    "            y_true.append(list(y_test[0]))\n",
    "            # report progress\n",
    "            print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "           # Calculate roc_auc on the hold out dataset\n",
    "        print('auc: %.3f' % roc_auc_score(y_true, y_pred))       \n",
    "        print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Logistic Regression 1 3 \n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202]\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "        # prepare data (log transform)\n",
    "        from scipy.stats import skew\n",
    "        skewness = X.apply(lambda x: skew(x))\n",
    "        skewed_feats = skewness[skewness > 0.75]\n",
    "        skewed_feats = skewed_feats.index\n",
    "        minimum = X[skewed_feats].min(axis = 1).min()\n",
    "        skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "        X = X.to_numpy()\n",
    "        \n",
    "        # Logistic Regression 01 (log transform, Scaler, RFECV, LR)\n",
    "        pipe01 = Pipeline([('scaler',MinMaxScaler()),\n",
    "                         ('features',RFECV(estimator = LogisticRegression(solver='liblinear'),cv =3, scoring ='roc_auc')),\n",
    "                         ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "        param01 = {'lr__penalty':['l1','l2'],\n",
    "                'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR01:')\n",
    "        self_LR_pipe(X,y,pipe01,param01)\n",
    "        print('*'*30)\n",
    "        \n",
    "        # Logistic Regression 03 (log transform, scaler, SelectFromModel, LR)\n",
    "        selector = SelectFromModel(estimator=LogisticRegression(solver = 'liblinear'))\n",
    "        pipe_lr03 = Pipeline([ \n",
    "                ('scaler',MinMaxScaler()),('features',selector),\n",
    "            ('lr',LogisticRegression(solver = 'liblinear',random_state=42))]) \n",
    "        param_lr03 = {'features__max_features':[20,30,40],\n",
    "                        'lr__penalty':['l1', 'l2'],\n",
    "                        'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR03:')               \n",
    "        self_LR_pipe(X,y,pipe=pipe_lr03,param=param_lr03)\n",
    "        print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Random Forest01\n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202].to_numpy()\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "        # Random forest 01 (scaler, pca + mututal info classifier, RandomForest)\n",
    "        pca = PCA(n_components = 0.9) \n",
    "        selection = SelectKBest(mutual_info_classif,k=10)\n",
    "        combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "        pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                         ('features',combined_features),\n",
    "                         ('rf',RandomForestClassifier(random_state=42))]) \n",
    "        param = {'features__univ_select__k':[10,20,30,40],\n",
    "                 'rf__max_features':['sqrt','log2'],\n",
    "                 'rf__n_estimators':[50,100,1000,2000]} \n",
    "        print('*'*50)\n",
    "        print('Below is the results from the RF01:')\n",
    "        cv_outer = LeaveOneOut()\n",
    "        y_true,y_pred = list(),list()\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "            y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            ## inner loop for feature selection and hyperparameter tuning \n",
    "            cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=28)\n",
    "            result = GridSearchCV(pipe, param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "            best_param = result.best_estimator_\n",
    "            # evaluate model on the hold out evaluation dataset\n",
    "            yhat_proba = list(best_param.predict_proba(X_test)[:,1])# reture the probability of predicting '1'\n",
    "            y_pred.append(yhat_proba)\n",
    "            y_true.append(list(y_test[0]))\n",
    "            # report progress\n",
    "            print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "            # Calculate roc_auc on the hold out dataset\n",
    "        print('auc: %.3f' % roc_auc_score(y_true, y_pred))\n",
    "        print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CN and AU Random Forest 2\n",
    "# df_cancer ={}\n",
    "# AUC = dict()\n",
    "# ACU = dict()\n",
    "# SEN = dict()\n",
    "# SPE = dict()\n",
    "# for index, rad in enumerate(rads):\n",
    "#     for key,value in Side_View.items():\n",
    "#         cancer_sort = value.sort_values(by = rad)\n",
    "#         cancer_sort_copy = cancer_sort.copy()\n",
    "#         cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[1,3,0])\n",
    "#         cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "#         print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "#         X = cancer_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "#         y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "#         over = SMOTE(sampling_strategy=\"minority\",random_state=2)\n",
    "#         X,y = over.fit_resample(X,y)\n",
    "        \n",
    "      \n",
    "#         # Random Forest 02 (scaler, SelectFromModel, RandomForest)\n",
    "#         selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "#         pipe = Pipeline([ #('scaler',MinMaxScaler()),\n",
    "#                          ('features',selector),\n",
    "#                          ('rf',RandomForestClassifier(random_state=42))]) \n",
    "#         param = {'features__max_features':[5,10,15],\n",
    "#                          'rf__max_features':['sqrt'], \n",
    "#                      'rf__n_estimators':[50,100,1000] } \n",
    "#         print('*'*50)\n",
    "#         print('Below is the results from the RF02:')\n",
    "#         Dict,AUC_score,Accuracy,sensitivity1,specificity1  = self_rf_pipe(X,y,pipe,param)  \n",
    "#         print('*'*50)\n",
    "#         df_cancer[key]=Dict    \n",
    "#         AUC[key]=AUC_score\n",
    "#         ACU[key]=Accuracy\n",
    "#         SEN[key]=sensitivity1\n",
    "#         SPE[key]=specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS Case_based_CC_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.769, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.778, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.801, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.861, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.884, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.963, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.907, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.991, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=1.000, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.852, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.741, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.852, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.713, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.736, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.759, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.5714285714285714\n",
      "The AUC for this view is : 0.7678571428571428\n",
      "The ACC for this view is : 0.6666666666666666\n",
      "The sen for this view is : 0.75\n",
      "The spe for this view is : 0.5714285714285714\n",
      "ThIS IS Case_based_MLO_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.611, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.630, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.565, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.528, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.671, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.519, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.560, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.602, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      "Specificity :  0.5714285714285714\n",
      "The AUC for this view is : 0.06249999999999999\n",
      "The ACC for this view is : 0.26666666666666666\n",
      "The sen for this view is : 0.0\n",
      "The spe for this view is : 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# FOR CHINENSE READERS\n",
    "rad = rads[0]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    cancer_sort = value.sort_values(by = rad)\n",
    "    cancer_sort_copy = cancer_sort.copy()\n",
    "    cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[0,3,1])\n",
    "    cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==0],cancer_sort_copy[cancer_sort_copy['percentile']==1]],axis = 0)\n",
    "\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = cancer_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "    y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "    pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,15],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    \n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.3333333333333333\n",
      "Sensitivity is : 0.0\n",
      "Specificity is : 0.7142857142857143\n",
      "AUC for normal pipeline for Australian readers is 0.491\n",
      "Confidence interval for the score: [0.143 - 0.857]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(2,-1)\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Australian readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS Case_based_CC_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.731, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.741, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.778, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.704, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.630, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.796, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.713, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.759, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.639, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.759, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.792, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.755, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.787, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.852, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.713, cfg={'features__max_features': 15, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.685, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      "Specificity :  0.3333333333333333\n",
      "The AUC for this view is : 0.6000000000000001\n",
      "The ACC for this view is : 0.5625\n",
      "The sen for this view is : 0.7\n",
      "The spe for this view is : 0.3333333333333333\n",
      "ThIS IS Case_based_MLO_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.620, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.574, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.611, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.556, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.611, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.611, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.630, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.713, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.630, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.593, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.755, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.657, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.639, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.833, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.699, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.657, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      "Specificity :  0.3333333333333333\n",
      "The AUC for this view is : 0.4833333333333333\n",
      "The ACC for this view is : 0.625\n",
      "The sen for this view is : 0.8\n",
      "The spe for this view is : 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "## FOR AUSTRALIAN READERS\n",
    "rad = rads[1]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    cancer_sort = value.sort_values(by = rad)\n",
    "    cancer_sort_copy = cancer_sort.copy()\n",
    "    cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[0,3,1])\n",
    "    cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==0],cancer_sort_copy[cancer_sort_copy['percentile']==1]],axis = 0)\n",
    "\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = cancer_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "    y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "    pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,15],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.5625\n",
      "Sensitivity is : 0.6\n",
      "Specificity is : 0.5\n",
      "AUC for normal pipeline for Australian readers is 0.483\n",
      "Confidence interval for the score: [0.146 - 0.855]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(2,-1)\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Australian readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Based\n",
    "### Cancer Lesion Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_feat_location = pd.read_csv(\"/Users/jessy/Desktop/笔记本/Radiomics Data Analysis/diffScore w_out 10 dep Cn/case_feat_location_diffScore/whole_feat.csv\")\n",
    "cancer_location = case_feat_location.merge(lesion_side.rename(columns={\"LesionNumber\":\"LesionNum\"}), how='left',on = [\"CaseName\",\"LesionNum\"])\n",
    "CC_CL_LC = cancer_location.loc[cancer_location[\"Side\"]==cancer_location[\"LesionSide\"],:].loc[cancer_location[\"View\"]==\"'CC'\",:]\n",
    "MLO_CL_LC = cancer_location.loc[cancer_location[\"Side\"]==cancer_location[\"LesionSide\"],:].loc[cancer_location[\"View\"]==\"'MLO'\",:]\n",
    "\n",
    "CC_CL_LC = CC_CL_LC[CC_CL_LC.LesionNum!=2]\n",
    "MLO_CL_LC = MLO_CL_LC[MLO_CL_LC.LesionNum!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer lesion side \n",
    "Side_View = {}\n",
    "Side_View = {'Loc_based_CC_CancerLesion':CC_CL_LC,\n",
    "             'Loc_based_MLO_CancerLesion': MLO_CL_LC}\n",
    "rads = ['diffScore.CN','diffScore.AU']\n",
    "#Bins = [[0,0.2,0.58,1],[0,0.5,0.77,1]] \n",
    "# For location \n",
    "# Chinese [0,0,25,0.7,1]\n",
    "# Au [0,0.5625,0.7500,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Logistic Regression 02\n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202]\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "        # prepare data (log transform)\n",
    "        from scipy.stats import skew\n",
    "        skewness = X.apply(lambda x: skew(x))\n",
    "        skewed_feats = skewness[skewness > 0.75]\n",
    "        skewed_feats = skewed_feats.index\n",
    "        minimum = X[skewed_feats].min(axis = 1).min()\n",
    "        skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "        X = X.to_numpy()  \n",
    "\n",
    "        \n",
    "        combined_features = \n",
    "        pipe_lr2 = Pipeline([\n",
    "            ('scaler',MinMaxScaler()),\n",
    "            ('features',combined_features),\n",
    "            ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "        param_lr2 = {'features__univ_select__k':[5,10,20,30],\n",
    "                    'lr__penalty':['l1', 'l2'],\n",
    "                    'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR02:')\n",
    "        cv_outer = LeaveOneOut()\n",
    "        y_true,y_pred = list(),list()\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            X_train_, X_test = X[train_ix, :], X[test_ix, :]\n",
    "            y_train_, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            # inner loop for feature selection and hyperparameter tuning \n",
    "            cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "            result = GridSearchCV(pipe_lr2, param_lr2, cv = cv_inner, n_jobs= -1,scoring = 'roc_auc',refit=True).fit(X_train_, y_train_.ravel())\n",
    "            best_param = result.best_estimator_\n",
    "            # evaluate model on the hold out validation set\n",
    "            yhat_proba = list(best_param.predict_proba(X_test)[:,1])\n",
    "            y_pred.append(yhat_proba)\n",
    "            y_true.append(list(y_test[0]))\n",
    "            # report progress\n",
    "            print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "           # Calculate roc_auc on the hold out dataset\n",
    "        print('auc: %.3f' % roc_auc_score(y_true, y_pred))       \n",
    "        print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Logistic Regression 1 3 \n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202]\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "        # prepare data (log transform)\n",
    "        from scipy.stats import skew\n",
    "        skewness = X.apply(lambda x: skew(x))\n",
    "        skewed_feats = skewness[skewness > 0.75]\n",
    "        skewed_feats = skewed_feats.index\n",
    "        minimum = X[skewed_feats].min(axis = 1).min()\n",
    "        skewed_feats = np.log1p(X[skewed_feats]+ abs(minimum))\n",
    "        X = X.to_numpy()\n",
    "        \n",
    "        # Logistic Regression 01 (log transform, Scaler, RFECV, LR)\n",
    "        pipe01 = Pipeline([('scaler',MinMaxScaler()),\n",
    "                         ('features',RFECV(estimator = LogisticRegression(solver='liblinear'),cv =3, scoring ='roc_auc')),\n",
    "                         ('lr',LogisticRegression(solver='liblinear',random_state=42))])\n",
    "        param01 = {'lr__penalty':['l1','l2'],\n",
    "                'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR01:')\n",
    "        self_LR_pipe(X,y,pipe01,param01)\n",
    "        print('*'*30)\n",
    "        \n",
    "        # Logistic Regression 03 (log transform, scaler, SelectFromModel, LR)\n",
    "        selector = SelectFromModel(estimator=LogisticRegression(solver = 'liblinear'))\n",
    "        pipe_lr03 = Pipeline([ \n",
    "                ('scaler',MinMaxScaler()),('features',selector),\n",
    "            ('lr',LogisticRegression(solver = 'liblinear',random_state=42))]) \n",
    "        param_lr03 = {'features__max_features':[20,30,40],\n",
    "                        'lr__penalty':['l1', 'l2'],\n",
    "                        'lr__C':[1000,100,10,1.0,0.1,0.01,0.001]} \n",
    "        print('*'*30)\n",
    "        print('Below is the results from the LR03:')               \n",
    "        self_LR_pipe(X,y,pipe=pipe_lr03,param=param_lr03)\n",
    "        print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN and AU Random Forest01\n",
    "for index, rad in enumerate(rads):\n",
    "    for key,value in Side_View.items():\n",
    "        cancer_sort = value.sort_values(by = rad)\n",
    "        cancer_sort_copy = cancer_sort.copy()\n",
    "        cancer_sort_copy['percentile'] = pd.cut(cancer_sort_copy[rad], Bins[index], labels=[1,3,0])\n",
    "        cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "        print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "        X = cancer_sort_drop.loc[:,0:202].to_numpy()\n",
    "        y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "        \n",
    "        # Random forest 01 (scaler, pca + mututal info classifier, RandomForest)\n",
    "        pca = PCA(n_components = 0.9) \n",
    "        selection = SelectKBest(mutual_info_classif,k=10)\n",
    "        combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "        pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                         ('features',combined_features),\n",
    "                         ('rf',RandomForestClassifier(random_state=42))]) \n",
    "        param = {'features__univ_select__k':[10,20,30,40],\n",
    "                 'rf__max_features':['sqrt','log2'],\n",
    "                 'rf__n_estimators':[50,100,1000,2000]} \n",
    "        print('*'*50)\n",
    "        print('Below is the results from the RF01:')\n",
    "        cv_outer = LeaveOneOut()\n",
    "        y_true,y_pred = list(),list()\n",
    "        for train_ix, test_ix in cv_outer.split(X):\n",
    "            X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "            y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            ## inner loop for feature selection and hyperparameter tuning \n",
    "            cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "            result = GridSearchCV(pipe, param, cv = cv_inner, scoring = 'roc_auc',n_jobs = -1,refit=True).fit(X_train, y_train.ravel())\n",
    "            best_param = result.best_estimator_\n",
    "            # evaluate model on the hold out evaluation dataset\n",
    "            yhat_proba = list(best_param.predict_proba(X_test)[:,1])# reture the probability of predicting '1'\n",
    "            y_pred.append(yhat_proba)\n",
    "            y_true.append(list(y_test[0]))\n",
    "            # report progress\n",
    "            print('>est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
    "            # Calculate roc_auc on the hold out dataset\n",
    "        print('auc: %.3f' % roc_auc_score(y_true, y_pred))\n",
    "        print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CN and AU Random Forest 2\n",
    "# df_location = {}\n",
    "# AUC = dict()\n",
    "# ACU = dict()\n",
    "# SEN = dict()\n",
    "# SPE = dict()\n",
    "# for index, rad in enumerate(rads):\n",
    "#     for key,value in Side_View.items():\n",
    "#         cancer_sort = value.sort_values(by = rad)\n",
    "#         cancer_sort_copy = cancer_sort.copy()\n",
    "#         cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[1,3,0])\n",
    "#         cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "#         print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "#         X = cancer_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "#         y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "#         over = SMOTE(sampling_strategy=\"minority\",random_state=2)\n",
    "#         X,y = over.fit_resample(X,y)\n",
    "\n",
    "\n",
    "#         # Random Forest 02 (scaler, SelectFromModel, RandomForest)\n",
    "#         selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "#         pipe = Pipeline([ #('scaler',MinMaxScaler()),\n",
    "#                          ('features',selector),\n",
    "#                          ('rf',RandomForestClassifier(random_state=42))]) \n",
    "#         param = {'features__max_features':[5,10,15],\n",
    "#                          'rf__max_features':['sqrt'], \n",
    "#                      'rf__n_estimators':[50,100,1000] } \n",
    "#         print('*'*50)\n",
    "#         print('Below is the results from the RF02:')\n",
    "#         Dict,AUC_score,Accuracy,sensitivity1,specificity1  = self_rf_pipe(X,y,pipe,param)  \n",
    "#         print('*'*50) \n",
    "#         df_location[key]=Dict\n",
    "#         AUC[key]=AUC_score\n",
    "#         ACU[key]=Accuracy\n",
    "#         SEN[key]=sensitivity1\n",
    "#         SPE[key]=specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS Loc_based_CC_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.676, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.556, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.537, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.583, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.574, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.546, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.542, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.750, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.861, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.569, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.667, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.639, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.722, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      "Specificity :  0.2857142857142857\n",
      "The AUC for this view is : 0.30612244897959184\n",
      "The ACC for this view is : 0.42857142857142855\n",
      "The sen for this view is : 0.5714285714285714\n",
      "The spe for this view is : 0.2857142857142857\n",
      "ThIS IS Loc_based_MLO_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.704, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.546, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.509, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.616, cfg={'features__max_features': 15, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.556, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.505, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.537, cfg={'features__max_features': 15, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.829, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 1000}\n",
      ">est=0.569, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.556, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      "Specificity :  0.14285714285714285\n",
      "The AUC for this view is : 0.17346938775510204\n",
      "The ACC for this view is : 0.35714285714285715\n",
      "The sen for this view is : 0.5714285714285714\n",
      "The spe for this view is : 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "# FOR CHINENSE READERS\n",
    "rad = rads[0]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    cancer_sort = value.sort_values(by = rad)\n",
    "    cancer_sort_copy = cancer_sort.copy()\n",
    "    cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[0,3,1])\n",
    "    cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = cancer_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "    y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "    pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,15],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    \n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "    PRED.append(pre_label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.2857142857142857\n",
      "Sensitivity is : 0.2857142857142857\n",
      "Specificity is : 0.2857142857142857\n",
      "AUC for normal pipeline for Chinese readers is 0.133\n",
      "Confidence interval for the score: [0.000 - 0.356]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(2,-1)\n",
    "# get the maximum prob among predictions of the four views\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Chinese readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIS IS Loc_based_CC_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.519, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.620, cfg={'features__max_features': 5, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.556, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.514, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.556, cfg={'features__max_features': 10, 'rf__max_depth': 10, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      ">est=0.569, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 1.0, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.620, cfg={'features__max_features': 10, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 50}\n",
      "Specificity :  0.0\n",
      "The AUC for this view is : 0.09259259259259259\n",
      "The ACC for this view is : 0.4\n",
      "The sen for this view is : 0.6666666666666666\n",
      "The spe for this view is : 0.0\n",
      "ThIS IS Loc_based_MLO_CancerLesion SIDE VIEW \n",
      "\n",
      "\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.537, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 1.0, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.528, cfg={'features__max_features': 5, 'rf__max_depth': 5, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.556, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 1000}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.620, cfg={'features__max_features': 10, 'rf__max_depth': 1, 'rf__max_samples': 0.5, 'rf__n_estimators': 100}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      ">est=0.500, cfg={'features__max_features': 5, 'rf__max_depth': 1, 'rf__max_samples': 0.1, 'rf__n_estimators': 50}\n",
      "Specificity :  0.0\n",
      "The AUC for this view is : 0.0\n",
      "The ACC for this view is : 0.5333333333333333\n",
      "The sen for this view is : 0.8888888888888888\n",
      "The spe for this view is : 0.0\n"
     ]
    }
   ],
   "source": [
    "# FOR AU READERS\n",
    "rad = rads[1]\n",
    "PROB = []\n",
    "PRED = []\n",
    "TRUE = []\n",
    "for key,value in Side_View.items():\n",
    "    cancer_sort = value.sort_values(by = rad)\n",
    "    cancer_sort_copy = cancer_sort.copy()\n",
    "    cancer_sort_copy['percentile'] = pd.qcut(cancer_sort_copy[rad], 3, labels=[0,3,1])\n",
    "    cancer_sort_drop = pd.concat([cancer_sort_copy[cancer_sort_copy['percentile']==1],cancer_sort_copy[cancer_sort_copy['percentile']==0]],axis = 0)\n",
    "\n",
    "    print('ThIS IS {} SIDE VIEW'.format(key),'\\n\\n')\n",
    "    X = cancer_sort_drop.loc[:,\"feat.Whole.1\":\"feat.Whole.203\"].to_numpy()\n",
    "    y = cancer_sort_drop['percentile'].to_numpy().reshape(-1,1)\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(),max_features = 20)\n",
    "    pipe = Pipeline([ ('scaler',MinMaxScaler()),\n",
    "                     ('features',selector),\n",
    "                     ('rf',RandomForestClassifier(random_state=42))]) \n",
    "    param = {\"features__max_features\":[5,10,15],\n",
    "            \"rf__max_depth\":[1,5,10],\n",
    "             \"rf__max_samples\":[0.1,0.5,1.0],\n",
    "            \"rf__n_estimators\":[50,100,1000] } \n",
    "    probability, pre_label,true_value = pipeline_evaluate(X,y,pipe, param)\n",
    "    \n",
    "    AUC_score = roc_auc_score(true_value, probability)\n",
    "    Accuracy = accuracy_score(true_value,pre_label)\n",
    "    \n",
    "    cm1 = confusion_matrix(true_value,pre_label)\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "    print(\"The AUC for this view is :\", AUC_score)\n",
    "    print(\"The ACC for this view is :\", Accuracy)\n",
    "    print(\"The sen for this view is :\", sensitivity1)\n",
    "    print(\"The spe for this view is :\", specificity1)\n",
    "    \n",
    "    PROB.extend(probability)\n",
    "#     PRED.append(pre_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray is : 0.3333333333333333\n",
      "Sensitivity is : 0.5555555555555556\n",
      "Specificity is : 0.0\n",
      "AUC for normal pipeline for Australian readers is 0.074\n",
      "Confidence interval for the score: [0.000 - 0.273]\n"
     ]
    }
   ],
   "source": [
    "PROB = np.array(PROB).reshape(2,-1)\n",
    "# get the maximum prob among predictions of the four views\n",
    "PROB_max = np.max(PROB,axis=0)\n",
    "\n",
    "label = []\n",
    "for i in range(len(PROB_max)):\n",
    "    if PROB_max[i]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "         \n",
    "# calculate sensitivity, specificity, accuracy\n",
    "Accuracy = accuracy_score(true_value,label)\n",
    "cm1 = confusion_matrix(true_value,label)\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(\"Accuray is :\",Accuracy )\n",
    "print(\"Sensitivity is :\", sensitivity1)\n",
    "print(\"Specificity is :\", specificity1)\n",
    "\n",
    "# calculate pipeline AUC\n",
    "AUC_score = roc_auc_score(true_value, PROB_max)\n",
    "print(\"AUC for normal pipeline for Australian readers is {:.3f}\".format(AUC_score))\n",
    "\n",
    "# get confidence interval\n",
    "n_bootstraps = 2000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bootstrapped_scores = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.randint(0, len(PROB_max), len(PROB_max))\n",
    "    if len(np.unique(np.array(true_value)[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = roc_auc_score(np.array(true_value)[indices], PROB_max[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "#     print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "\n",
    "sorted_scores = np.array(bootstrapped_scores)\n",
    "sorted_scores.sort()\n",
    "confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "    confidence_lower, confidence_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
